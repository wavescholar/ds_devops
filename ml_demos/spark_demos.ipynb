{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/03 14:02:48 WARN Utils: Your hostname, bruce-NUC8i7BEH resolves to a loopback address: 127.0.1.1; using 192.168.1.9 instead (on interface wlp0s20f3)\n",
      "21/09/03 14:02:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/bruce/anaconda3/envs/snowflakes/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/09/03 14:02:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.140800\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Pi\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# You can examine the full config\n",
    "spark.sparkContext.getConf().getAll()\n",
    "\n",
    "ss=SparkSession.builder.master(\"local\").getOrCreate()\n",
    "\n",
    "sc=spark.sparkContext.getConf()\n",
    "print(ss.version)\n",
    "\n",
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "def f(_):\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "partitions = 2\n",
    "n = 10000 * partitions\n",
    "count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/03 14:03:23 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/09/03 14:03:23 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.07286177035421133,0.03824751451611519,0.014087121188640596]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [0.00646721278982503,-0.006787749805620738,-0.017842545041016167]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [-0.009297877550125122,0.07595592737197876,-0.06726421769708395]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Word2VecExample\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))\n",
    "# $example off$\n",
    "\n",
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|      features|label|\n",
      "+--------------+-----+\n",
      "|[0.0,0.0,18.0]|  1.0|\n",
      "|[1.0,0.0,12.0]|  0.0|\n",
      "|[0.0,1.0,15.0]|  0.0|\n",
      "+--------------+-----+\n",
      "\n",
      "Coefficients: [0.0,0.0,0.06093901400932439]\n",
      "Intercept: -0.5807518768065326\n",
      "numIterations: 10\n",
      "objectiveHistory: [0.5, 0.48867004169790873, 0.4497217673145075, 0.4443566163778985, 0.44361756323553725, 0.4435157581691582, 0.44350173445293756, 0.44349980267661604, 0.44349953657313135, 0.4434994999171975, 0.44349949486781826]\n",
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|  0.4838496246386935|\n",
      "|-0.15051629130536004|\n",
      "|-0.33333333333333315|\n",
      "+--------------------+\n",
      "\n",
      "RMSE: 0.350179\n",
      "r2: 0.448185\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"RFormulaExample\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# $example on$\n",
    "dataset = spark.createDataFrame(\n",
    "    [(7, \"US\", 18, 1.0),\n",
    "     (8, \"CA\", 12, 0.0),\n",
    "     (9, \"NZ\", 15, 0.0)],\n",
    "    [\"id\", \"country\", \"hour\", \"clicked\"])\n",
    "\n",
    "formula = RFormula(\n",
    "    formula=\"clicked ~ country + hour\",\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\")\n",
    "\n",
    "output = formula.fit(dataset).transform(dataset)\n",
    "output.select(\"features\", \"label\").show()\n",
    "# $example off$\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(output)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n",
    "\n",
    "\n",
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}